# OpenCodeReasoning-2

## License
This project is licensed under the [Apache License 2.0](LICENSE).




## Overview

OpenCodeReasoning-2 is the largest reasoning-based synthetic dataset for coding, designed for supervised fine-tuning (SFT) tasks in code completion and code critique. It includes 1.4M Python samples and 1.1M C++ samples across 34,799 unique competitive programming questions. The dataset is available for both commercial and non-commercial use.



## Dataset Details

Owner(s): NVIDIA Corporation, Benjamin Sinzore

Creation Date: March 2025 - May 2025

Modification Date: May 2025

Release Date: May 15, 2025

Version: 1.0

Data Developer: NVIDIA

Use Case: Training Large Language Models (LLMs) for code generation and critique.




## Data Sources

The dataset aggregates problems from the following sources:

CodeForces

TACO

APPS

CodeContests

open-r1/codeforces

Note: Test splits from CodeContests and open-r1/codeforces are excluded. Solution responses are generated by R1, and critique responses by QwQ.


## Data Characterization

Collection Method: Hybrid (Automated, Synthetic)

Labeling Method: Hybrid (Automated, Synthetic)

## Intended Usage

OpenCodeReasoning-2 is intended to advance open models for code-related tasks. Users may freely train models with the dataset but must verify that the dataset license suits their intended purpose.



from tqdm import tqdm
from datasets import load_dataset

# Load source datasets
```hf_datasets = {
    "taco": load_dataset("BAAI/TACO", trust_remote_code=True),```
    "apps": load_dataset("codeparrot/apps", trust_remote_code=True),```
    "code_contests": load_dataset("deepmind/code_contests"),```
    "open-r1/codeforces": load_dataset("open-r1/codeforces")```
}```

# Function to extract questions
def get_question(ds_name, split, index):
    benchmark = hf_datasets[ds_name][split][int(index)]
    if ds_name == "code_contests":
        if not benchmark["description"]:
            return None
        return benchmark["description"]
    elif ds_name in ["taco", "apps"]:
        return benchmark["question"]
    elif ds_name == "open-r1/codeforces":
        if not benchmark["description"]:
            return None
        question = benchmark["description"]
        if benchmark["input_format"]:
            question += "\n\n**Input**\n\n" + benchmark["input_format"]
        if benchmark["output_format"]:
            question += "\n\n**Output**\n\n" + benchmark["output_format"]
        if benchmark["examples"]:
            question += "\n\n**Examples**"
            for example in benchmark["examples"]:
                if "input" in example:
                    question += "\n\n**Input**\n\n" + example["input"]
                if "output" in example:
                    question += "\n\n**Output**\n\n" + example["output"]
        if benchmark["note"]:
            question += "\n\n**Note**\n\n" + benchmark["note"]
        return question
    return None

# Process OpenCodeReasoning-2 dataset
ocr2_dataset = load_dataset("nvidia/OpenCodeReasoning-2")
for ocr2_ds in [ocr2_dataset["python"], ocr2_dataset["cpp"]]:
    for ocr2_ds_item in tqdm(ocr2_ds):
        assert ocr2_ds_item["dataset"] in ["taco", "apps", "code_contests", "open-r1/codeforces"]
        ds_name, ds_split, ds_index = ocr2_ds_item["dataset"], ocr2_ds_item["split"], int(ocr2_ds_item["index"])
        question = get_question(ds_name, ds_split, ds_index)
        assert question is not None
        assert ocr2_ds_item["question"] == "-"
        ocr2_ds_item["question"] = question






## Ethical Considerations

NVIDIA and Benjamin Sinzore emphasize Trustworthy AI as a shared responsibility. Developers must adhere to the terms of service and ensure the dataset meets industry-specific requirements while addressing potential misuse.
